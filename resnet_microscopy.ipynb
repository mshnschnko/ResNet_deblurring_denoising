{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53LdDL1Hh3ej",
        "outputId": "8736080b-399c-4bec-a3a7-04d57bed4fcb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:275: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 154s 2s/step - loss: 0.0034 - val_loss: 2.0759e-04\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 93s 942ms/step - loss: 1.2379e-04 - val_loss: 9.0405e-05\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 94s 953ms/step - loss: 6.3259e-05 - val_loss: 7.0619e-05\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 93s 939ms/step - loss: 4.8536e-05 - val_loss: 3.6969e-05\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 93s 937ms/step - loss: 3.4977e-05 - val_loss: 2.3200e-05\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 93s 941ms/step - loss: 3.1575e-05 - val_loss: 2.2806e-05\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 95s 956ms/step - loss: 2.5772e-05 - val_loss: 1.5771e-05\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 94s 953ms/step - loss: 2.2605e-05 - val_loss: 1.2882e-05\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 93s 937ms/step - loss: 1.8325e-05 - val_loss: 2.2246e-05\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 92s 933ms/step - loss: 1.7573e-05 - val_loss: 1.9448e-05\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 92s 928ms/step - loss: 1.4390e-05 - val_loss: 1.7175e-05\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 93s 938ms/step - loss: 1.5857e-05 - val_loss: 1.2377e-05\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 92s 929ms/step - loss: 1.1691e-05 - val_loss: 1.1363e-05\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 93s 940ms/step - loss: 1.1244e-05 - val_loss: 8.2165e-06\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 92s 928ms/step - loss: 1.1723e-05 - val_loss: 1.4419e-05\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 92s 929ms/step - loss: 1.1526e-05 - val_loss: 8.9758e-06\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 93s 934ms/step - loss: 8.9589e-06 - val_loss: 7.9597e-06\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 92s 933ms/step - loss: 8.6231e-06 - val_loss: 1.0638e-05\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 93s 939ms/step - loss: 7.0675e-06 - val_loss: 8.2031e-06\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - ETA: 0s - loss: 8.6602e-06"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "import os, random\n",
        "import numpy as np\n",
        "from skimage.draw import line\n",
        "\n",
        "import imp\n",
        "import matplotlib.pyplot as plt\n",
        "# from scipy.misc import imageio.imwrite\n",
        "\n",
        "from scipy.ndimage.filters import convolve\n",
        "# import sol5_utils\n",
        "from skimage.color import rgb2gray\n",
        "import imageio\n",
        "import tifffile as tff\n",
        "from PIL import Image\n",
        "# from scipy.misc import imageio\n",
        "# from tensorflow.keras.layers import Input , Dense , Conv2D , Activation , Add\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.optimizers import adam_v2\n",
        "# from tensorflow.python.keras.layers import Input , Dense , Conv2D , Activation , Add\n",
        "# from tensorflow.python.keras.models import Model\n",
        "# from tensorflow.python.keras.optimizers import adam_v2\n",
        "from keras import Model, Input\n",
        "from keras.layers import Conv2D, Activation, Add, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import preprocessing\n",
        "import random\n",
        "\n",
        "def relpath(path):\n",
        "    \"\"\"Returns the relative path to the script's location\n",
        "\n",
        "    Arguments:\n",
        "    path -- a string representation of a path.\n",
        "    \"\"\"\n",
        "    return os.path.join(os.path.dirname(\"resnet_by_jew.ipynb\"), path)\n",
        "\n",
        "def list_images(path, use_shuffle=True):\n",
        "    \"\"\"Returns a list of paths to images found at the specified directory.\n",
        "\n",
        "    Arguments:\n",
        "    path -- path to a directory to search for images.\n",
        "    use_shuffle -- option to shuffle order of files. Uses a fixed shuffled order.\n",
        "    \"\"\"\n",
        "    def is_image(filename):\n",
        "        return os.path.splitext(filename)[-1][1:].lower() in ['jpg', 'png', 'tif', 'tiff']\n",
        "    images = list(map(lambda x: os.path.join(path, x), filter(is_image, os.listdir(path))))\n",
        "    # Shuffle with a fixed seed without affecting global state\n",
        "    if use_shuffle:\n",
        "        s = random.getstate()\n",
        "        random.seed(1234)\n",
        "        random.shuffle(images)\n",
        "        random.setstate(s)\n",
        "    return images\n",
        "\n",
        "def images_for_denoising():\n",
        "    \"\"\"Returns a list of image paths to be used for image denoising in Ex5\"\"\"\n",
        "    return list_images(relpath('drive/MyDrive/ResNet datasets/image_dataset/new_beads'), True)\n",
        "\n",
        "def images_for_deblurring():\n",
        "    \"\"\"Returns a list of image paths to be used for text deblurring in Ex5\"\"\"\n",
        "    return list_images(relpath('drive/MyDrive/ResNet datasets/image_dataset/new_beads'), True)\n",
        "\n",
        "# For those who wish to experiment...\n",
        "def images_for_super_resolution():\n",
        "    \"\"\"Returns a list of image paths to be used for image super-resolution in Ex5\"\"\"\n",
        "    return list_images(relpath('drive/MyDrive/ResNet datasets/image_dataset/new_beads'), True)\n",
        "\n",
        "def motion_blur_kernel(kernel_size, angle):\n",
        "    \"\"\"Returns a 2D image kernel for motion blur effect.\n",
        "\n",
        "    Arguments:\n",
        "    kernel_size -- the height and width of the kernel. Controls strength of blur.\n",
        "    angle -- angle in the range [0, np.pi) for the direction of the motion.\n",
        "    \"\"\"\n",
        "    if kernel_size % 2 == 0:\n",
        "        raise ValueError('kernel_size must be an odd number!')\n",
        "    if angle < 0 or angle > np.pi:\n",
        "        raise ValueError('angle must be between 0 (including) and pi (not including)')\n",
        "    norm_angle = 2.0 * angle / np.pi\n",
        "    if norm_angle > 1:\n",
        "        norm_angle = 1 - norm_angle\n",
        "    half_size = kernel_size // 2\n",
        "    if abs(norm_angle) == 1:\n",
        "        p1 = (half_size, 0)\n",
        "        p2 = (half_size, kernel_size-1)\n",
        "    else:\n",
        "        alpha = np.tan(np.pi * 0.5 * norm_angle)\n",
        "        if abs(norm_angle) <= 0.5:\n",
        "            p1 = (2*half_size, half_size - int(round(alpha * half_size)))\n",
        "            p2 = (kernel_size-1 - p1[0], kernel_size-1 - p1[1])\n",
        "        else:\n",
        "            alpha = np.tan(np.pi * 0.5 * (1-norm_angle))\n",
        "            p1 = (half_size - int(round(alpha * half_size)), 2*half_size)\n",
        "            p2 = (kernel_size - 1 - p1[0], kernel_size-1 - p1[1])\n",
        "    rr, cc = line(p1[0], p1[1], p2[0], p2[1])\n",
        "    kernel = np.zeros((kernel_size, kernel_size), dtype=np.float64)\n",
        "    kernel[rr, cc] = 1.0\n",
        "    kernel /= kernel.sum()\n",
        "    return kernel\n",
        "\n",
        "\n",
        "\n",
        "#'From ex.1 import read_image. read_image reads a file from the project folder and representation value of 1/2 from BW or colored image'\n",
        "# From Ex.1 read_image\n",
        "def read_image(filename,representation):\n",
        "    if representation not in [1,2]:                  # In case of invalid representation entry\n",
        "        raise (\"representation index error\")\n",
        "    # image = imageio.imread(filename)                    #  Loading RGB image.\n",
        "    image = tff.imread(filename)\n",
        "    if image.ndim == 3:\n",
        "        if representation == 1:  # Turns to gray, normalize and return image.\n",
        "            image = rgb2gray(image)\n",
        "            max_value = np.amax(np.amax(image))\n",
        "            if max_value > 1:\n",
        "                image = (image / 255).astype(np.float64)\n",
        "            return image\n",
        "        if representation == 2:                     #  Normalize intensity and return image\n",
        "            max_value = np.amax([np.amax(image[:, :, 0]), np.amax(image[:, :, 1]), np.amax(image[:, :, 2])])\n",
        "            if max_value > 1:\n",
        "                image = (image / 255).astype(np.float64)\n",
        "            return image\n",
        "    if image.ndim == 2:\n",
        "        max_value = np.amax(np.amax(image))\n",
        "        if max_value > 1:\n",
        "            image = (image / 255).astype(np.float64)\n",
        "        return image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'Inputs are:'\n",
        "'Filename is a list of filenames corresponds to clean images'\n",
        "'batch_size is the size of each batch of an image for each iteration of the SGD'\n",
        "'Corruption func recives an image as a singal argument, returns a randomly corrupted image'\n",
        "'crop_size is a tuple (height,width) specifaying the crop size'\n",
        "'Output is (source_batch,target_patch)'\n",
        "'each of them in the shape of (batch_size,height,width,1) Target corresponds to clean batches wheres the source are the noisy ones'\n",
        "'To improve taking the random number'\n",
        "\n",
        "\n",
        "def load_dataset(filenames, batch_size,corruption_func,crop_size):\n",
        "    # Prepare dictionaries,arrays and generate random indices for images.\n",
        "    dictionary_images = {}\n",
        "    dictionary_shapes = {}\n",
        "    N_filenames = len(filenames)\n",
        "    shape_list = []\n",
        "    filenames_ind_rand = np.random.randint(0, N_filenames,batch_size)\n",
        "    # filenames_ind_rand = np.random.randint(530,1000,batch_size)\n",
        "    # filenames_ind_rand = np.random.randint(435,477,batch_size)\n",
        "\n",
        "    source_batch = np.zeros([batch_size,crop_size[0],crop_size[1],1])\n",
        "    target_batch = np.zeros([batch_size,crop_size[0],crop_size[1],1])\n",
        "    # Load dictionaries: one for images the other for shapes\n",
        "    # Reduces time to call read_image\n",
        "    # Shapes dictionary is ['filename'] = max_row-3*crop_size_row ,  max_col-3*crop_size_col\n",
        "    while True:\n",
        "        for i in np.arange(0,batch_size):\n",
        "            filename_ind_rand = filenames_ind_rand[i]\n",
        "            if filenames[filename_ind_rand] not in dictionary_images.keys():\n",
        "                dictionary_images[filenames[filename_ind_rand]] = read_image(filenames[filename_ind_rand],1)\n",
        "                row_tot = dictionary_images[filenames[filename_ind_rand]].shape[0]\n",
        "                col_tot = dictionary_images[filenames[filename_ind_rand]].shape[1]\n",
        "                dictionary_shapes[filenames[filename_ind_rand]] =  (row_tot - 3*crop_size[0],col_tot - 3*crop_size[1])\n",
        "\n",
        "        # Use the dictionary to generate a random patch in every image\n",
        "        for i in np.arange(0,batch_size):\n",
        "            # Load image and subtracted shape\n",
        "            filename = filenames[filenames_ind_rand[i]]\n",
        "            image = dictionary_images[filename]\n",
        "            max_row = dictionary_shapes[filename][0]\n",
        "            max_col = dictionary_shapes[filename][1]\n",
        "            # Generate random number in a cropped image\n",
        "            rand_row = np.random.randint(3 * crop_size[0], max_row - 3 * crop_size[0])\n",
        "            rand_col = np.random.randint(3 * crop_size[1], max_col - 3 * crop_size[1])\n",
        "            # Get patch for cropped image\n",
        "            indices_r_i = rand_row - np.floor(3*crop_size[0]/2).astype(np.int32)\n",
        "            indices_r_f = rand_row + np.floor(3*crop_size[0]/2).astype(np.int32)\n",
        "            indices_c_i = rand_col - np.floor(3*crop_size[1]/2).astype(np.int32)\n",
        "            indices_c_f = rand_col + np.floor(3*crop_size[1]/2).astype(np.int32)\n",
        "\n",
        "            # shift the indexes to the whole picture\n",
        "            rand_row_shift = np.random.randint(-np.floor(3 * crop_size[0])/2, np.floor(3 * crop_size[0])/2)\n",
        "            rand_col_shift = np.random.randint(-np.floor(3 * crop_size[1])/2, np.floor(3 * crop_size[0])/2)\n",
        "            img_cropped_3 = image[indices_r_i+rand_row_shift:indices_r_f+rand_row_shift,indices_c_i+rand_col_shift:indices_c_f+rand_col_shift]\n",
        "\n",
        "            # plt.figure(3);\n",
        "            # plt.imshow(img_cropped_3);\n",
        "            # plt.show()\n",
        "\n",
        "            # Corrupted image\n",
        "            img_corrupted = corruption_func(img_cropped_3)\n",
        "            # Generating random in sub part of the image\n",
        "            rand_row_cropped = np.random.randint(crop_size[0], img_corrupted.shape[0] - crop_size[0])\n",
        "            rand_col_cropped = np.random.randint(crop_size[1], img_corrupted.shape[1] - crop_size[1])\n",
        "            indices_r_i = rand_row_cropped - np.floor(crop_size[0]/2).astype(np.int32)\n",
        "            indices_r_f = rand_row_cropped + np.floor(crop_size[0]/2).astype(np.int32)\n",
        "            indices_c_i = rand_col_cropped - np.floor(crop_size[1]/2).astype(np.int32)\n",
        "            indices_c_f = rand_col_cropped + np.floor(crop_size[1]/2).astype(np.int32)\n",
        "\n",
        "            # Target batch generator\n",
        "            rand_row_shift_1 = np.random.randint(-np.floor(1 * crop_size[0])/2, np.floor(1 * crop_size[0])/2)\n",
        "            rand_col_shift_1 = np.random.randint(-np.floor(1 * crop_size[1])/2, np.floor(1 * crop_size[0])/2)\n",
        "            img_cropped_1 = img_cropped_3[indices_r_i+rand_row_shift_1:indices_r_f+rand_row_shift_1,indices_c_i+rand_col_shift_1:indices_c_f+rand_col_shift_1]- 0.5\n",
        "            # img_cropped_1 = img_cropped_3[indices_r_i:indices_r_f,indices_c_i:indices_c_f] - 0.5\n",
        "            img_cropped_sub_res = np.reshape(img_cropped_1,[img_cropped_1.shape[0],img_cropped_1.shape[1],1])\n",
        "            target_batch[i, :, :, :] = img_cropped_sub_res\n",
        "\n",
        "            # plt.figure(1);\n",
        "            # plt.imshow(img_cropped_1);\n",
        "\n",
        "            # Source batch generator\n",
        "            # img_corrupted_cropped_sub = img_corrupted[indices_r_i:indices_r_f,indices_c_i:indices_c_f] - 0.5\n",
        "            img_corrupted_cropped_sub = img_corrupted[indices_r_i+rand_row_shift_1:indices_r_f+rand_row_shift_1,indices_c_i+rand_col_shift_1:indices_c_f+rand_col_shift_1]- 0.5\n",
        "            img_corrupted_cropped_sub_res = np.reshape(img_corrupted_cropped_sub,[img_corrupted_cropped_sub.shape[0],img_corrupted_cropped_sub.shape[1],1])\n",
        "            source_batch[i,:,:,:] = img_corrupted_cropped_sub_res\n",
        "\n",
        "            # plt.figure(2);\n",
        "            # plt.imshow(img_corrupted_cropped_sub);\n",
        "            # plt.show()\n",
        "\n",
        "        # plt.figure;\n",
        "        # plt.imshow(target_batch[i,:,:,:]);\n",
        "        # plt.show()\n",
        "        # plt.figure;\n",
        "        # plt.imshow(img_corrupted_cropped_sub);\n",
        "        # plt.show()\n",
        "        # print(dictionary_shapes)\n",
        "        yield (source_batch , target_batch)\n",
        "\n",
        "'Each resblock is conv, relu conv add (input,conv) and relu on (input+conv)'\n",
        "def resblock(input_tensor, num_channels):\n",
        "    conv = Conv2D (num_channels,(3,3),padding='same')(input_tensor)\n",
        "    relu = Activation('relu')(conv)\n",
        "    conv2 = Conv2D (num_channels,(3,3),padding='same')(relu)\n",
        "    add = Add()([input_tensor,conv2])\n",
        "    output_tensor = Activation('relu')(add)\n",
        "    return output_tensor\n",
        "\n",
        "'Build model as specified in the HW '\n",
        "def build_nn_model(height,width,num_channels,num_res_blocks):\n",
        "    inp = Input(shape=(height,width,1))\n",
        "    conv = Conv2D (num_channels,(3,3),padding='same')(inp)\n",
        "    block_out = Activation('relu')(conv)\n",
        "    for i in np.arange(0,num_res_blocks):\n",
        "        block_out = resblock(block_out,num_channels)\n",
        "    conv_m2 = Conv2D (1,(3,3),padding='same')(block_out)\n",
        "    add_m1 = Add()([inp,conv_m2])\n",
        "    # add_m1 = Dense(1)(add_m1)\n",
        "    model = Model(inputs =inp,outputs = add_m1)\n",
        "    return model\n",
        "\n",
        "def train_model(model,images,corruption_func,batch_size,steps_per_epoch,num_epochs,num_valid_samples):\n",
        "# Split images to train and valid:\n",
        "# Get valid and train size, 80% train 20% valid\n",
        "    filenames_nump = np.array(images)\n",
        "    images_length = len(images)\n",
        "    train_size = np.floor(images_length * 0.8).astype(np.int64)\n",
        "    valid_size = images_length - train_size\n",
        "    list1 = [True] * train_size\n",
        "    list2 = [False] * valid_size\n",
        "    listcomb = list1 + list2\n",
        "    rand_indices = np.random.choice(listcomb, images_length, replace=False)\n",
        "    train_indices = np.argwhere(rand_indices == True)\n",
        "    valid_indices = np.argwhere(rand_indices == False)\n",
        "    train_samples = filenames_nump[train_indices][:,0]\n",
        "    valid_samples = filenames_nump[valid_indices][:,0]\n",
        "    train_samples_list = (train_samples)\n",
        "    valid_samples_list = (valid_samples)\n",
        "# Define generators\n",
        "    train_generator = load_dataset(train_samples_list, batch_size, corruption_func, model.input_shape[1:3])\n",
        "    valid_generator = load_dataset(valid_samples_list, batch_size, corruption_func, model.input_shape[1:3])\n",
        "# Train model\n",
        "    model.compile(optimizer= Adam(beta_2 = 0.9),loss='mean_squared_error')\n",
        "    # model.fit_generator(train_generator,steps_per_epoch,num_epochs,validation_data=valid_generator,validation_steps=num_valid_samples)\n",
        "    hist = model.fit_generator(train_generator, steps_per_epoch, num_epochs, validation_data=valid_generator,validation_steps=num_valid_samples)\n",
        "    return hist\n",
        "\n",
        "\n",
        "'restore image using predict'\n",
        "def restore_image(corrupted_image,base_model):\n",
        "    inp = Input(shape=(corrupted_image.shape[0],corrupted_image.shape[1],1))\n",
        "    base_mod = base_model(inp)\n",
        "    new_model = Model(inputs=inp,outputs= base_mod)\n",
        "    image_shifted = np.reshape(corrupted_image-0.5,[corrupted_image.shape[0],corrupted_image.shape[1],1]) # последний параметр можно поменять на 3 и будет цветная картинка :)\n",
        "    image_predicted = new_model.predict(image_shifted[np.newaxis,...])[0].astype(np.float64)\n",
        "    image_restored = image_predicted + 0.5\n",
        "    return np.reshape(image_restored.clip(0,1),[corrupted_image.shape[0],corrupted_image.shape[1]])\n",
        "\n",
        "\n",
        "\n",
        "'Adds gaussian noise to original image, randomly choosen between min_sigma to sigma'\n",
        "def add_gaussian_noise(image,min_sigma,max_sigma):\n",
        "    rand_sigma = np.random.uniform(min_sigma,max_sigma)\n",
        "    gauss_noise = np.random.normal(0.0 , rand_sigma,image.shape)\n",
        "    image_corrupted_rounded = np.round((gauss_noise + image)*255)\n",
        "    image_corrupted_01 = np.clip(image_corrupted_rounded / 255 , 0,1)\n",
        "    return image_corrupted_01\n",
        "\n",
        "def learn_denoising_model(num_res_blocks = 5,quick_mode = False):\n",
        "    filenames = images_for_denoising()\n",
        "    height = 36 ; width = 36 #\n",
        "    channels_num = 48 #было 48\n",
        "    model = build_nn_model(height,width,channels_num,num_res_blocks)\n",
        "    corruption_func =  lambda x: add_gaussian_noise(x,0,0.2)\n",
        "    batch_size = 100\n",
        "    steps_per_epoch = 100\n",
        "    epoch_num = 50\n",
        "    num_valid_samples = 1000\n",
        "    if quick_mode == True:\n",
        "        batch_size = 10\n",
        "        steps_per_epoch = 3\n",
        "        epoch_num = 2\n",
        "        num_valid_samples = 30\n",
        "\n",
        "    hist = train_model(model,filenames,corruption_func,batch_size,steps_per_epoch,epoch_num,num_valid_samples)\n",
        "    return model , hist\n",
        "\n",
        "    # train_model(model,filenames,corruption_func,batch_size,steps_per_epoch,epoch_num,num_valid_samples)\n",
        "    # return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_motion_blur(image,kernel_size,angle):\n",
        "    mask = motion_blur_kernel(kernel_size,angle)\n",
        "    img_corrupted = convolve(image,mask,mode='reflect',cval=0)\n",
        "    return img_corrupted\n",
        "\n",
        "def random_motion_blur(image,list_of_kernel_sizes):\n",
        "    angle = random.uniform(0,np.pi)\n",
        "    kernel_size = np.random.choice(list_of_kernel_sizes)\n",
        "    img_corrupted = add_motion_blur(image,kernel_size,angle)\n",
        "    img_corrupted_255 = np.round(img_corrupted * 255)\n",
        "    img_corrupted_01 = np.clip(img_corrupted_255 / 255,0,1)\n",
        "    # plt.figure(1) ; plt.imshow(image) ; plt.show()\n",
        "    return img_corrupted_01.astype(np.float64)\n",
        "\n",
        "def learn_deblurring_model(num_res_blocks=5, quick_mode = False):\n",
        "    filenames = images_for_deblurring()\n",
        "    # filenames.remove(filenames[434])\n",
        "    # filenames.remove(filenames[529 - 1])\n",
        "    height = 36 ; width = 36\n",
        "    channels_num = 32 #было 32\n",
        "    # num_res_blocks = 5\n",
        "    model = build_nn_model(height,width,channels_num,num_res_blocks)\n",
        "    corruption_func =  lambda x: random_motion_blur(x,[7])\n",
        "    batch_size = 100\n",
        "    steps_per_epoch = 100\n",
        "    epoch_num = 50 # сделать 5!\n",
        "    num_valid_samples = 1000\n",
        "    if quick_mode == True:\n",
        "        batch_size = 10\n",
        "        steps_per_epoch = 3\n",
        "        epoch_num = 2\n",
        "        num_valid_samples = 30\n",
        "    hist = train_model(model,filenames,corruption_func,batch_size,steps_per_epoch,epoch_num,num_valid_samples)\n",
        "    return  model , hist\n",
        "\n",
        "    # train_model(model,filenames,corruption_func,batch_size,steps_per_epoch,epoch_num,num_valid_samples)\n",
        "    # return  model\n",
        "\n",
        "def res_visual(history):\n",
        "    # acc = history.history['accuracy']\n",
        "    # val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs = range(len(loss))\n",
        "\n",
        "    # plt.plot(epochs, acc, 'b', label = 'Training acc')\n",
        "    # plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
        "    # plt.title('Training and validation accuracy')\n",
        "    # plt.legend()\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    plt.plot(epochs, loss, 'b', label = 'Training loss')\n",
        "    plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    img = read_image(\"drive/MyDrive/ResNet datasets/low_res/bead_00.tif\", 1)\n",
        "    # print(type(img))\n",
        "    model, hist = learn_denoising_model(quick_mode = False)\n",
        "    res_img = restore_image(img, model)\n",
        "    # res_img = restore_image(img, learn_deblurring_model(quick_mode = False))\n",
        "    # res_ = np.append(res_img, 1)\n",
        "    # res_img = res_\n",
        "    preprocessing.image.save_img(\"bead_00_dnr.tif\", np.reshape(res_img, [36, 36, 1]))\n",
        "    res_visual(hist)\n",
        "    # im = Image.fromarray(res_img.astype('uint8'))\n",
        "    \n",
        "    # # im.show()\n",
        "    # im = im.convert(\"L\")\n",
        "    # im.save(\"testRdn.jpeg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYEQ2khJcx6y"
      },
      "outputs": [],
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "from skimage.draw import line\n",
        "\n",
        "def relpath(path):\n",
        "    \"\"\"Returns the relative path to the script's location\n",
        "\n",
        "    Arguments:\n",
        "    path -- a string representation of a path.\n",
        "    \"\"\"\n",
        "    return os.path.join(os.path.dirname(__file__), path)\n",
        "\n",
        "def list_images(path, use_shuffle=True):\n",
        "    \"\"\"Returns a list of paths to images found at the specified directory.\n",
        "\n",
        "    Arguments:\n",
        "    path -- path to a directory to search for images.\n",
        "    use_shuffle -- option to shuffle order of files. Uses a fixed shuffled order.\n",
        "    \"\"\"\n",
        "    def is_image(filename):\n",
        "        return os.path.splitext(filename)[-1][1:].lower() in ['jpg', 'png']\n",
        "    images = list(map(lambda x: os.path.join(path, x), filter(is_image, os.listdir(path))))\n",
        "    # Shuffle with a fixed seed without affecting global state\n",
        "    if use_shuffle:\n",
        "        s = random.getstate()\n",
        "        random.seed(1234)\n",
        "        random.shuffle(images)\n",
        "        random.setstate(s)\n",
        "    return images\n",
        "\n",
        "def images_for_denoising():\n",
        "    \"\"\"Returns a list of image paths to be used for image denoising in Ex5\"\"\"\n",
        "    return list_images(relpath('drive/MyDrive/ResNet datasets/image_dataset/train'), True)\n",
        "\n",
        "def images_for_deblurring():\n",
        "    \"\"\"Returns a list of image paths to be used for text deblurring in Ex5\"\"\"\n",
        "    return list_images(relpath('drive/MyDrive/ResNet datasets/text_dataset/train'), True)\n",
        "\n",
        "# For those who wish to experiment...\n",
        "def images_for_super_resolution():\n",
        "    \"\"\"Returns a list of image paths to be used for image super-resolution in Ex5\"\"\"\n",
        "    return list_images(relpath('drive/MyDrive/ResNet datasets/image_dataset/train'), True)\n",
        "\n",
        "def motion_blur_kernel(kernel_size, angle):\n",
        "    \"\"\"Returns a 2D image kernel for motion blur effect.\n",
        "\n",
        "    Arguments:\n",
        "    kernel_size -- the height and width of the kernel. Controls strength of blur.\n",
        "    angle -- angle in the range [0, np.pi) for the direction of the motion.\n",
        "    \"\"\"\n",
        "    if kernel_size % 2 == 0:\n",
        "        raise ValueError('kernel_size must be an odd number!')\n",
        "    if angle < 0 or angle > np.pi:\n",
        "        raise ValueError('angle must be between 0 (including) and pi (not including)')\n",
        "    norm_angle = 2.0 * angle / np.pi\n",
        "    if norm_angle > 1:\n",
        "        norm_angle = 1 - norm_angle\n",
        "    half_size = kernel_size // 2\n",
        "    if abs(norm_angle) == 1:\n",
        "        p1 = (half_size, 0)\n",
        "        p2 = (half_size, kernel_size-1)\n",
        "    else:\n",
        "        alpha = np.tan(np.pi * 0.5 * norm_angle)\n",
        "        if abs(norm_angle) <= 0.5:\n",
        "            p1 = (2*half_size, half_size - int(round(alpha * half_size)))\n",
        "            p2 = (kernel_size-1 - p1[0], kernel_size-1 - p1[1])\n",
        "        else:\n",
        "            alpha = np.tan(np.pi * 0.5 * (1-norm_angle))\n",
        "            p1 = (half_size - int(round(alpha * half_size)), 2*half_size)\n",
        "            p2 = (kernel_size - 1 - p1[0], kernel_size-1 - p1[1])\n",
        "    rr, cc = line(p1[0], p1[1], p2[0], p2[1])\n",
        "    kernel = np.zeros((kernel_size, kernel_size), dtype=np.float64)\n",
        "    kernel[rr, cc] = 1.0\n",
        "    kernel /= kernel.sum()\n",
        "    return kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvS9ctbfc86f"
      },
      "outputs": [],
      "source": [
        "import imp\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# from scipy.misc import imageio.imwrite\n",
        "\n",
        "from scipy.ndimage.filters import convolve\n",
        "import sol5_utils\n",
        "# import sol5_utils\n",
        "from skimage.color import rgb2gray\n",
        "import imageio\n",
        "# from scipy.misc import imageio\n",
        "# from tensorflow.keras.layers import Input , Dense , Conv2D , Activation , Add\n",
        "# from tensorflow.keras.models import Model\n",
        "# from tensorflow.keras.optimizers import adam_v2\n",
        "# from tensorflow.python.keras.layers import Input , Dense , Conv2D , Activation , Add\n",
        "# from tensorflow.python.keras.models import Model\n",
        "# from tensorflow.python.keras.optimizers import adam_v2\n",
        "from keras import Model, Input\n",
        "from keras.layers import Conv2D, Activation, Add, Dense\n",
        "from keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "\n",
        "#'From ex.1 import read_image. read_image reads a file from the project folder and representation value of 1/2 from BW or colored image'\n",
        "# From Ex.1 read_image\n",
        "def read_image(filename,representation):\n",
        "    if representation not in [1,2]:                  # In case of invalid representation entry\n",
        "        raise (\"representation index error\")\n",
        "    image = imageio.imread(filename)                    #  Loading RGB image.\n",
        "    if image.ndim == 3:\n",
        "        if representation == 1:  # Turns to gray, normalize and return image.\n",
        "            image = rgb2gray(image)\n",
        "            max_value = np.amax(np.amax(image))\n",
        "            if max_value > 1:\n",
        "                image = (image / 255).astype(np.float64)\n",
        "            return image\n",
        "        if representation == 2:                     #  Normalize intensity and return image\n",
        "            max_value = np.amax([np.amax(image[:, :, 0]), np.amax(image[:, :, 1]), np.amax(image[:, :, 2])])\n",
        "            if max_value > 1:\n",
        "                image = (image / 255).astype(np.float64)\n",
        "            return image\n",
        "    if image.ndim == 2:\n",
        "        max_value = np.amax(np.amax(image))\n",
        "        if max_value > 1:\n",
        "            image = (image / 255).astype(np.float64)\n",
        "        return image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'Inputs are:'\n",
        "'Filename is a list of filenames corresponds to clean images'\n",
        "'batch_size is the size of each batch of an image for each iteration of the SGD'\n",
        "'Corruption func recives an image as a singal argument, returns a randomly corrupted image'\n",
        "'crop_size is a tuple (height,width) specifaying the crop size'\n",
        "'Output is (source_batch,target_patch)'\n",
        "'each of them in the shape of (batch_size,height,width,1) Target corresponds to clean batches wheres the source are the noisy ones'\n",
        "'To improve taking the random number'\n",
        "\n",
        "\n",
        "def load_dataset(filenames, batch_size,corruption_func,crop_size):\n",
        "    # Prepare dictionaries,arrays and generate random indices for images.\n",
        "    dictionary_images = {}\n",
        "    dictionary_shapes = {}\n",
        "    N_filenames = len(filenames)\n",
        "    shape_list = []\n",
        "    filenames_ind_rand = np.random.randint(0, N_filenames,batch_size)\n",
        "    # filenames_ind_rand = np.random.randint(530,1000,batch_size)\n",
        "    # filenames_ind_rand = np.random.randint(435,477,batch_size)\n",
        "\n",
        "    source_batch = np.zeros([batch_size,crop_size[0],crop_size[1],1])\n",
        "    target_batch = np.zeros([batch_size,crop_size[0],crop_size[1],1])\n",
        "    # Load dictionaries: one for images the other for shapes\n",
        "    # Reduces time to call read_image\n",
        "    # Shapes dictionary is ['filename'] = max_row-3*crop_size_row ,  max_col-3*crop_size_col\n",
        "    while True:\n",
        "        for i in np.arange(0,batch_size):\n",
        "            filename_ind_rand = filenames_ind_rand[i]\n",
        "            if filenames[filename_ind_rand] not in dictionary_images.keys():\n",
        "                dictionary_images[filenames[filename_ind_rand]] = read_image(filenames[filename_ind_rand],1)\n",
        "                row_tot = dictionary_images[filenames[filename_ind_rand]].shape[0]\n",
        "                col_tot = dictionary_images[filenames[filename_ind_rand]].shape[1]\n",
        "                dictionary_shapes[filenames[filename_ind_rand]] =  (row_tot - 3*crop_size[0],col_tot - 3*crop_size[1])\n",
        "\n",
        "        # Use the dictionary to generate a random patch in every image\n",
        "        for i in np.arange(0,batch_size):\n",
        "            # Load image and subtracted shape\n",
        "            filename = filenames[filenames_ind_rand[i]]\n",
        "            image = dictionary_images[filename]\n",
        "            max_row = dictionary_shapes[filename][0]\n",
        "            max_col = dictionary_shapes[filename][1]\n",
        "            # Generate random number in a cropped image\n",
        "            rand_row = np.random.randint(3 * crop_size[0], max_row - 3 * crop_size[0])\n",
        "            rand_col = np.random.randint(3 * crop_size[1], max_col - 3 * crop_size[1])\n",
        "            # Get patch for cropped image\n",
        "            indices_r_i = rand_row - np.floor(3*crop_size[0]/2).astype(np.int32)\n",
        "            indices_r_f = rand_row + np.floor(3*crop_size[0]/2).astype(np.int32)\n",
        "            indices_c_i = rand_col - np.floor(3*crop_size[1]/2).astype(np.int32)\n",
        "            indices_c_f = rand_col + np.floor(3*crop_size[1]/2).astype(np.int32)\n",
        "\n",
        "            # shift the indexes to the whole picture\n",
        "            rand_row_shift = np.random.randint(-np.floor(3 * crop_size[0])/2, np.floor(3 * crop_size[0])/2)\n",
        "            rand_col_shift = np.random.randint(-np.floor(3 * crop_size[1])/2, np.floor(3 * crop_size[0])/2)\n",
        "            img_cropped_3 = image[indices_r_i+rand_row_shift:indices_r_f+rand_row_shift,indices_c_i+rand_col_shift:indices_c_f+rand_col_shift]\n",
        "\n",
        "            # plt.figure(3);\n",
        "            # plt.imshow(img_cropped_3);\n",
        "            # plt.show()\n",
        "\n",
        "            # Corrupted image\n",
        "            img_corrupted = corruption_func(img_cropped_3)\n",
        "            # Generating random in sub part of the image\n",
        "            rand_row_cropped = np.random.randint(crop_size[0], img_corrupted.shape[0] - crop_size[0])\n",
        "            rand_col_cropped = np.random.randint(crop_size[1], img_corrupted.shape[1] - crop_size[1])\n",
        "            indices_r_i = rand_row_cropped - np.floor(crop_size[0]/2).astype(np.int32)\n",
        "            indices_r_f = rand_row_cropped + np.floor(crop_size[0]/2).astype(np.int32)\n",
        "            indices_c_i = rand_col_cropped - np.floor(crop_size[1]/2).astype(np.int32)\n",
        "            indices_c_f = rand_col_cropped + np.floor(crop_size[1]/2).astype(np.int32)\n",
        "\n",
        "            # Target batch generator\n",
        "            rand_row_shift_1 = np.random.randint(-np.floor(1 * crop_size[0])/2, np.floor(1 * crop_size[0])/2)\n",
        "            rand_col_shift_1 = np.random.randint(-np.floor(1 * crop_size[1])/2, np.floor(1 * crop_size[0])/2)\n",
        "            img_cropped_1 = img_cropped_3[indices_r_i+rand_row_shift_1:indices_r_f+rand_row_shift_1,indices_c_i+rand_col_shift_1:indices_c_f+rand_col_shift_1]- 0.5\n",
        "            # img_cropped_1 = img_cropped_3[indices_r_i:indices_r_f,indices_c_i:indices_c_f] - 0.5\n",
        "            img_cropped_sub_res = np.reshape(img_cropped_1,[img_cropped_1.shape[0],img_cropped_1.shape[1],1])\n",
        "            target_batch[i, :, :, :] = img_cropped_sub_res\n",
        "\n",
        "            # plt.figure(1);\n",
        "            # plt.imshow(img_cropped_1);\n",
        "\n",
        "            # Source batch generator\n",
        "            # img_corrupted_cropped_sub = img_corrupted[indices_r_i:indices_r_f,indices_c_i:indices_c_f] - 0.5\n",
        "            img_corrupted_cropped_sub = img_corrupted[indices_r_i+rand_row_shift_1:indices_r_f+rand_row_shift_1,indices_c_i+rand_col_shift_1:indices_c_f+rand_col_shift_1]- 0.5\n",
        "            img_corrupted_cropped_sub_res = np.reshape(img_corrupted_cropped_sub,[img_corrupted_cropped_sub.shape[0],img_corrupted_cropped_sub.shape[1],1])\n",
        "            source_batch[i,:,:,:] = img_corrupted_cropped_sub_res\n",
        "\n",
        "            # plt.figure(2);\n",
        "            # plt.imshow(img_corrupted_cropped_sub);\n",
        "            # plt.show()\n",
        "\n",
        "        # plt.figure;\n",
        "        # plt.imshow(target_batch[i,:,:,:]);\n",
        "        # plt.show()\n",
        "        # plt.figure;\n",
        "        # plt.imshow(img_corrupted_cropped_sub);\n",
        "        # plt.show()\n",
        "        # print(dictionary_shapes)\n",
        "        yield (source_batch , target_batch)\n",
        "\n",
        "'Each resblock is conv, relu conv add (input,conv) and relu on (input+conv)'\n",
        "def resblock(input_tensor, num_channels):\n",
        "    conv = Conv2D (num_channels,(3,3),padding='same')(input_tensor)\n",
        "    relu = Activation('relu')(conv)\n",
        "    conv2 = Conv2D (num_channels,(3,3),padding='same')(relu)\n",
        "    add = Add()([input_tensor,conv2])\n",
        "    output_tensor = Activation('relu')(add)\n",
        "    return output_tensor\n",
        "\n",
        "'Build model as specified in the HW '\n",
        "def build_nn_model(height,width,num_channels,num_res_blocks):\n",
        "    inp = Input(shape=(height,width,1))\n",
        "    conv = Conv2D (num_channels,(3,3),padding='same')(inp)\n",
        "    block_out = Activation('relu')(conv)\n",
        "    for i in np.arange(0,num_res_blocks):\n",
        "        block_out = resblock(block_out,num_channels)\n",
        "    conv_m2 = Conv2D (1,(3,3),padding='same')(block_out)\n",
        "    add_m1 = Add()([inp,conv_m2])\n",
        "    # add_m1 = Dense(1)(add_m1)\n",
        "    model = Model(inputs =inp,outputs = add_m1)\n",
        "    return model\n",
        "\n",
        "def train_model(model,images,corruption_func,batch_size,steps_per_epoch,num_epochs,num_valid_samples):\n",
        "# Split images to train and valid:\n",
        "# Get valid and train size, 80% train 20% valid\n",
        "    filenames_nump = np.array(images)\n",
        "    images_length = len(images)\n",
        "    train_size = np.floor(images_length * 0.8).astype(np.int64)\n",
        "    valid_size = images_length - train_size\n",
        "    list1 = [True] * train_size\n",
        "    list2 = [False] * valid_size\n",
        "    listcomb = list1 + list2\n",
        "    rand_indices = np.random.choice(listcomb, images_length, replace=False)\n",
        "    train_indices = np.argwhere(rand_indices == True)\n",
        "    valid_indices = np.argwhere(rand_indices == False)\n",
        "    train_samples = filenames_nump[train_indices][:,0]\n",
        "    valid_samples = filenames_nump[valid_indices][:,0]\n",
        "    train_samples_list = (train_samples)\n",
        "    valid_samples_list = (valid_samples)\n",
        "# Define generators\n",
        "    train_generator = load_dataset(train_samples_list, batch_size, corruption_func, model.input_shape[1:3])\n",
        "    valid_generator = load_dataset(valid_samples_list, batch_size, corruption_func, model.input_shape[1:3])\n",
        "# Train model\n",
        "    model.compile(optimizer= Adam(beta_2 = 0.9),loss='mean_squared_error')\n",
        "    model.fit_generator(train_generator,steps_per_epoch,num_epochs,validation_data=valid_generator,validation_steps=num_valid_samples)\n",
        "    # hist = model.fit_generator(train_generator, steps_per_epoch, num_epochs, validation_data=valid_generator,validation_steps=num_valid_samples // batch_size)\n",
        "    # return hist\n",
        "\n",
        "\n",
        "'restore image using predict'\n",
        "def restore_image(corrupted_image,base_model):\n",
        "    inp = Input(shape=(corrupted_image.shape[0],corrupted_image.shape[1],1))\n",
        "    base_mod = base_model(inp)\n",
        "    new_model = Model(inputs=inp,outputs= base_mod)\n",
        "    image_shifted = np.reshape(corrupted_image-0.5,[corrupted_image.shape[0],corrupted_image.shape[1],1]) # последний параметр можно поменять на 3 и будет цветная картинка :)\n",
        "    image_predicted = new_model.predict(image_shifted[np.newaxis,...])[0].astype(np.float64)\n",
        "    image_restored = image_predicted + 0.5\n",
        "    return np.reshape(image_restored.clip(0,1),[corrupted_image.shape[0],corrupted_image.shape[1]])\n",
        "\n",
        "\n",
        "\n",
        "'Adds gaussian noise to original image, randomly choosen between min_sigma to sigma'\n",
        "def add_gaussian_noise(image,min_sigma,max_sigma):\n",
        "    rand_sigma = np.random.uniform(min_sigma,max_sigma)\n",
        "    gauss_noise = np.random.normal(0.0 , rand_sigma,image.shape)\n",
        "    image_corrupted_rounded = np.round((gauss_noise + image)*255)\n",
        "    image_corrupted_01 = np.clip(image_corrupted_rounded / 255 , 0,1)\n",
        "    return image_corrupted_01\n",
        "\n",
        "def learn_denoising_model(num_res_blocks = 5,quick_mode = False):\n",
        "    filenames = sol5_utils.images_for_denoising()\n",
        "    height = 24 ; width = 24 #\n",
        "    channels_num = 48\n",
        "    model = build_nn_model(height,width,channels_num,num_res_blocks)\n",
        "    corruption_func =  lambda x: add_gaussian_noise(x,0,0.2)\n",
        "    batch_size = 100\n",
        "    steps_per_epoch = 100\n",
        "    epoch_num = 5\n",
        "    num_valid_samples = 1000\n",
        "    if quick_mode == True:\n",
        "        batch_size = 10\n",
        "        steps_per_epoch = 3\n",
        "        epoch_num = 2\n",
        "        num_valid_samples = 30\n",
        "\n",
        "    # hist = train_model(model,filenames,corruption_func,batch_size,steps_per_epoch,epoch_num,num_valid_samples)\n",
        "    # return model , hist\n",
        "\n",
        "    train_model(model,filenames,corruption_func,batch_size,steps_per_epoch,epoch_num,num_valid_samples)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def add_motion_blur(image,kernel_size,angle):\n",
        "    mask = sol5_utils.motion_blur_kernel(kernel_size,angle)\n",
        "    img_corrupted = convolve(image,mask,mode='reflect',cval=0)\n",
        "    return img_corrupted\n",
        "\n",
        "def random_motion_blur(image,list_of_kernel_sizes):\n",
        "    angle = random.uniform(0,np.pi)\n",
        "    kernel_size = np.random.choice(list_of_kernel_sizes)\n",
        "    img_corrupted = add_motion_blur(image,kernel_size,angle)\n",
        "    img_corrupted_255 = np.round(img_corrupted * 255)\n",
        "    img_corrupted_01 = np.clip(img_corrupted_255 / 255,0,1)\n",
        "    # plt.figure(1) ; plt.imshow(image) ; plt.show()\n",
        "    return img_corrupted_01.astype(np.float64)\n",
        "\n",
        "def learn_deblurring_model(num_res_blocks=5, quick_mode = False):\n",
        "    filenames = sol5_utils.images_for_deblurring()\n",
        "    # filenames.remove(filenames[434])\n",
        "    # filenames.remove(filenames[529 - 1])\n",
        "    height = 24 ; width = 24\n",
        "    channels_num = 32\n",
        "    # num_res_blocks = 5\n",
        "    model = build_nn_model(height,width,channels_num,num_res_blocks)\n",
        "    corruption_func =  lambda x: random_motion_blur(x,[7])\n",
        "    batch_size = 100\n",
        "    steps_per_epoch = 100\n",
        "    epoch_num = 5 # сделать 5!\n",
        "    num_valid_samples = 1000\n",
        "    if quick_mode == True:\n",
        "        batch_size = 10\n",
        "        steps_per_epoch = 3\n",
        "        epoch_num = 2\n",
        "        num_valid_samples = 30\n",
        "    # hist = train_model(model,filenames,corruption_func,batch_size,steps_per_epoch,epoch_num,num_valid_samples)\n",
        "    # return  model , hist\n",
        "\n",
        "    train_model(model,filenames,corruption_func,batch_size,steps_per_epoch,epoch_num,num_valid_samples)\n",
        "    return  model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4ZPlRpDdiEo"
      },
      "outputs": [],
      "source": [
        "from sol5 import *\n",
        "from PIL import Image\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    img = read_image(\"low_res/img24wb.jpg\", 1)\n",
        "    print(type(img))\n",
        "    res_img = restore_image(img, learn_deblurring_model())\n",
        "    im = Image.fromarray(res_img)\n",
        "    im.show()\n",
        "    im = im.convert(\"L\")\n",
        "    im.save(\"img24wbR.jpg\")\n",
        "    # res_img.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "resnet_by_jew.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}